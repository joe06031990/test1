// Defines the start of a declarative Jenkins pipeline for restoring Grafana dashboards.
// WARNING: This pipeline runs automatically and will overwrite dashboards without confirmation.
pipeline {
    // Specifies that the pipeline can run on any available Jenkins agent.
    agent any

    // This pipeline is now configured to run automatically on a schedule.
    triggers {
        // 'H * * * *' means the job will run approximately every hour.
        cron('H * * * *')
    }

    // Defines environment variables that will be available throughout the pipeline.
    environment {
        // Securely loads the Grafana API key from Jenkins credentials.
        GRAFANA_API_KEY = credentials('GRAFANA_API_KEY')
        // The URL of your Grafana instance.
        GRAFANA_URL = "https://jstest2025.grafana.net"
        // The name of the root directory where backups are stored in the Git repository.
        GRAFANA_BACKUP_DIR = "playground_nonprd"
    }

    // Contains all the main work stages of the pipeline.
    stages {
        // The first stage: responsible for checking out code from your Git repository.
        stage('Checkout Git Repository') {
            steps {
                // The 'checkout' step clones or updates the specified Git repository and branch.
                // Uses 'scm' to automatically checkout the branch that triggered this build
                checkout scm
            }
        }

        // This stage runs the restore script.
        stage('Run Grafana Restore') {
            steps {
                script {
                    // Define the entire restore script as a Groovy multiline string.
                    def restoreScriptContent = '''
                        #!/bin/bash
                        # 'set -e' ensures the script will exit immediately if a command fails.
                        set -e
                        # 'set -x' prints each command to the log before it is executed, useful for debugging.
                        set -x

                        # Sanity checks to ensure required tools and variables are present.
                        if [ -z "${GRAFANA_URL}" ] || [ -z "${GRAFANA_API_KEY}" ] || [ -z "${GRAFANA_BACKUP_DIR}" ]; then
                            echo "Missing required environment variables (GRAFANA_URL, GRAFANA_API_KEY, GRAFANA_BACKUP_DIR)." >&2
                            exit 1
                        fi

                        if [ ! -d "${GRAFANA_BACKUP_DIR}" ]; then
                            echo "Error: Backup directory '${GRAFANA_BACKUP_DIR}' not found." >&2
                            exit 1
                        fi

                        # A function to parse simple JSON values from strings.
                        extract_json_value() {
                            local json="$1"
                            local key="$2"
                            echo "$json" | awk -F"[,:{}\\\"]" -v key="$key" '{
                                for (i=1; i<=NF; i++) {
                                    if ($i == key) {
                                        # The value is the next non-empty field
                                        for (j=i+1; j<=NF; j++) {
                                            if ($j != "" && $j !~ /^[ \t]*$/) {
                                                print $j;
                                                exit;
                                            }
                                        }
                                    }
                                }
                            }'
                        }

                        # Use an associative array to map folder titles to their IDs
                        declare -A FOLDER_IDS

                        # --- PASS 1: Load all existing folders from Grafana ---
                        echo "--- Pass 1: Loading existing Grafana folders ---"
                        # Fetch all folders and use sed to parse the result.
                        # Using process substitution to avoid creating a subshell for the while loop.
                        while IFS= read -r line; do
                            # Extract title and id from each line of JSON
                            title=$(echo "$line" | sed -n 's/.*"title":"\\([^"]*\\)".*/\\1/p')
                            id=$(echo "$line" | sed -n 's/.*"id":\\([0-9]*\\).*/\\1/p')

                            # Ensure both title and id were found before adding to the array
                            if [ -n "$title" ] && [ -n "$id" ]; then
                                echo "Found existing folder: '${title}' with ID: ${id}"
                                FOLDER_IDS["${title}"]="${id}"
                            fi
                        done < <(curl -s -H "Authorization: Bearer ${GRAFANA_API_KEY}" "${GRAFANA_URL}/api/search?type=dash-folder" | sed 's/},{/}\\n{/g')
                        echo "--- Finished loading existing folders ---"

                        # --- PASS 2: Restore all dashboards, creating folders as needed ---
                        echo "--- Pass 2: Restoring dashboards ---"
                        find "${GRAFANA_BACKUP_DIR}" -type f -name "*.json" | while read -r dashboard_path; do
                            echo "--- Processing dashboard: ${dashboard_path} ---"
                            
                            path_in_backup="${dashboard_path#${GRAFANA_BACKUP_DIR}/}"
                            
                            # This logic correctly handles nested subdirectories and the special 'playground' folder.
                            relative_dir=$(dirname "${path_in_backup}")
                            
                            folder_name=""
                            # This logic correctly handles the repository structure to generate the target folder name.
                            if [[ "${relative_dir}" == "." ]] || [[ "${relative_dir}" == "playground" ]]; then
                                # Dashboards at the root or in a 'playground' subdir go to the main 'playground' folder.
                                folder_name="playground"
                            else
                                # For all other subdirectories, create a prefixed folder name.
                                # First, strip a leading 'playground/' if it exists, to avoid duplication.
                                path_to_process=${relative_dir#playground/}
                                # Then, replace any remaining slashes with underscores for nested directories.
                                sanitized_path=${path_to_process//\\//_}
                                folder_name="playground_${sanitized_path}"
                            fi

                            # Check if the folder is already in our map. If not, create it.
                            if [ -z "${FOLDER_IDS[$folder_name]}" ]; then
                                echo "Folder '${folder_name}' not found. Creating it..." >&2
                                create_payload="{\\"title\\": \\"${folder_name}\\"}"
                                create_result=$(curl -s -X POST -H "Authorization: Bearer ${GRAFANA_API_KEY}" -H "Content-Type: application/json" --data "$create_payload" "${GRAFANA_URL}/api/folders")
                                
                                new_folder_id=$(extract_json_value "$create_result" "id")
                                
                                if [ -z "$new_folder_id" ]; then
                                    echo "Error: Failed to create folder '${folder_name}'. Response: ${create_result}" >&2
                                    continue
                                fi
                                echo "Successfully created folder '${folder_name}' with ID: ${new_folder_id}" >&2
                                FOLDER_IDS["${folder_name}"]="${new_folder_id}"
                            fi

                            TARGET_FOLDER_ID="${FOLDER_IDS[${folder_name}]}"
                            
                            if [ -z "$TARGET_FOLDER_ID" ]; then
                                echo "Error: Could not find a Folder ID for '${folder_name}'. Skipping dashboard."
                                continue
                            fi
                            
                            full_backup_json=$(cat "${dashboard_path}")
                            dashboard_object=$(echo "$full_backup_json" | sed 's/.*,"dashboard"://' | sed 's/}$//' | sed 's/"id":[0-9]*,//')

                            if [ -z "$dashboard_object" ]; then
                                echo "Error: Could not extract dashboard object from '${dashboard_path}'. Skipping."
                                continue
                            fi
                            
                            restore_payload=$(printf '{"dashboard": %s, "folderId": %s, "overwrite": true}' "$dashboard_object" "$TARGET_FOLDER_ID")
                            restore_result=$(curl -s -X POST -H "Authorization: Bearer ${GRAFANA_API_KEY}" -H "Content-Type: application/json" --data-raw "${restore_payload}" "${GRAFANA_URL}/api/dashboards/db")

                            status=$(extract_json_value "$restore_result" "status")
                            if [ "$status" == "success" ]; then
                                echo "Successfully restored dashboard from '${dashboard_path}'"
                            else
                                echo "Error restoring dashboard from '${dashboard_path}'. Response: ${restore_result}"
                            fi
                        done

                        echo "--- Grafana restore process complete. ---"
                    '''
                    
                    // Write the script content to a file in the workspace
                    writeFile(file: 'run_grafana_restore.sh', text: restoreScriptContent)
                    // Make the file executable
                    sh 'chmod +x run_grafana_restore.sh'
                    // Execute the script explicitly with bash to ensure compatibility
                    sh 'bash ./run_grafana_restore.sh'
                }
            }
        }
    }

    // The 'post' section defines actions that run at the end of the pipeline.
    post {
        // 'always' means this action will run regardless of whether the pipeline succeeded or failed.
        always {
            // Clean up the workspace
            deleteDir()
        }
    }
}
